{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/daychman/miniconda3/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "# model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/tempo/merged_clean.csv'\n",
    "definitions = np.load(\"/tempo/processed-files/definitions.npy\", allow_pickle=True).item()\n",
    "df = pd.read_csv(file)\n",
    "words_corr = df[['Word A', 'Word B']].to_numpy()\n",
    "sentences = df['Sentence'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "layers = [-4, -3, -2, -1]\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
    "\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein(word1, word2):\n",
    "    \"\"\"Calculate the Levenshtein distance between two words.\"\"\"\n",
    "    if len(word1) > len(word2):\n",
    "        word1, word2 = word2, word1\n",
    "    distances = range(len(word1) + 1)\n",
    "    for index2, letter2 in enumerate(word2):\n",
    "        distances_ = [index2 + 1]\n",
    "        for index1, letter1 in enumerate(word1):\n",
    "            if letter1 == letter2:\n",
    "                distances_.append(distances[index1])\n",
    "            else:\n",
    "                distances_.append(1 + min((distances[index1], distances[index1 + 1], distances_[-1])))\n",
    "        distances = distances_\n",
    "    return distances[-1]    \n",
    "\n",
    "# def levenshtein(word1, word2):\n",
    "#     return np.sum(np.array(list(map(lambda x: x[0] != x[1], zip(word1, word2)))))\n",
    "\n",
    "def get_word_idx(sent: str, word: str):\n",
    "    # word can be 1-3 words long. Find the closest match in the sentence using regex.\n",
    "    words = re.findall(r'\\w+', sent)\n",
    "    # find the closest match in the sentence using Levenshtein distance \n",
    "    word_idx = min(range(len(words)), key=lambda i: levenshtein(word, words[i]))\n",
    "    return word_idx\n",
    "\n",
    "\n",
    "def batch_word_vector_BERT(sent_list, word_list):\n",
    "    splitted_words_list = [definitions.get(word, word).split() for word in word_list]\n",
    "    token_ids_list = [[get_word_idx(sent, word) for word in words] for sent, words in zip(sent_list, splitted_words_list)]\n",
    "    \n",
    "    if isinstance(sent_list, np.ndarray):\n",
    "        sent_list = sent_list.tolist()\n",
    "\n",
    "    encoded_list = tokenizer(sent_list, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    token_ids = []\n",
    "    for i in range(len(token_ids_list)):\n",
    "        idx = encoded_list.word_ids(i)\n",
    "        token_ids.append([])\n",
    "        for j in range(len(idx)):\n",
    "            if idx[j] in token_ids_list[i]:\n",
    "                token_ids[i].append(j)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded_list)\n",
    "\n",
    "    # Get all hidden states\n",
    "    states = output.hidden_states\n",
    "    # Stack and sum all requested layers\n",
    "    output = torch.stack([states[i] for i in layers]).sum(0)\n",
    "    # Only select the tokens that constitute the requested word\n",
    "    word_tokens_output = [output[i][token_ids[i]] for i in range(len(output))]\n",
    "    word_vectors = [tensor.mean(0) for tensor in word_tokens_output]\n",
    "    v_list = [tensor.numpy() for tensor in word_vectors]\n",
    "    # convert a list of tensors to a numpy array\n",
    "    return np.array(v_list)\n",
    "\n",
    "\n",
    "    # return np.mean(word_vectors, axis=0)\n",
    "\n",
    "sent = \"The fluffy cat is a good pet.\"\n",
    "sent_list = [sent] * 200\n",
    "word_list = [\"fluffy cat\"] * 200\n",
    "v_list = batch_word_vector_BERT(sent_list, word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = '/tempo/merged_clean.csv'\n",
    "definitions = np.load(\"/tempo/processed-files/definitions.npy\", allow_pickle=True).item()\n",
    "df = pd.read_csv(file)\n",
    "words_corr = df[['Word A', 'Word B']].to_numpy()\n",
    "sentences = df['Sentence'].to_numpy()\n",
    "embeddings = np.load('/tempo/processed-files/embeddings.npy')\n",
    "labels = np.load('/tempo/processed-files/labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['We propose two throughput scaling options for any one-dimensional convolution kernel in programmable processors by adjusting the imprecision (distortion) of computation.']\n",
      "['distortion']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1920)\n",
      "1\n",
      "1920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def process_embeddings(start=0, end=len(embeddings)):\n",
    "    global embeddings, words_corr, sentences\n",
    "    # cut the embeddings to the desired range\n",
    "    _embeddings = embeddings[start:end]\n",
    "    _sentences = sentences[start:end]\n",
    "    _words_corr = words_corr[start:end]\n",
    "\n",
    "\n",
    "    concat_embeddigs = []\n",
    "\n",
    "    batch_size = 100\n",
    "    n = np.ceil(len(_embeddings)/batch_size).astype(int)\n",
    "    for i in tqdm(range(n)):\n",
    "        words_corr_batch = _words_corr[i*batch_size:(i+1)*batch_size]\n",
    "        words1, words2 = words_corr_batch[:, 0], words_corr_batch[:, 1]\n",
    "        sent_batch = _sentences[i*batch_size:(i+1)*batch_size]\n",
    "        embeddings_batch = _embeddings[i*batch_size:(i+1)*batch_size]\n",
    "        print(sent_batch)\n",
    "        print(words1)\n",
    "        \n",
    "        try :\n",
    "            vec1 = batch_word_vector_BERT(sent_batch, words1)\n",
    "            vec2 = batch_word_vector_BERT(sent_batch, words2)\n",
    "            concat_embeddigs.append(np.concatenate((vec1, vec2, embeddings_batch), axis=1))\n",
    "        except:\n",
    "            raise ValueError(\"Error in sentence: \" + str(i))\n",
    "    \n",
    "    concat_embeddigs = np.array(concat_embeddigs).reshape(-1, 1920)\n",
    "\n",
    "    np.save(f'concat_embeddings ({start}-{end})', concat_embeddigs)\n",
    "    return concat_embeddigs\n",
    "\n",
    "v = process_embeddings(start=1440239, end=1440240)\n",
    "print(v.shape)\n",
    "print(len(v))\n",
    "print(len(v[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/tempo/processed-files/concat_embeddings', concat_embeddigs)\n",
    "np.save('/tempo/processed-files/concat_labels', concat_labels)\n",
    "np.save('/tempo/processed-files/concat_sentences.npy', concat_sentences)\n",
    "np.save('/tempo/processed-files/concat_words.npy', concat_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d075635c49ac9fb9913723b1e0fe7df7567b1c4ee6ede4cef9a7e4fcacf814a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
